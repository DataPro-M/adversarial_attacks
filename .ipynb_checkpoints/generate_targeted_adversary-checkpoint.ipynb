{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing targeted adversarial attacks with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "\t# swap color channels, resize the input image, and add a batch\n",
    "\t# dimension\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\timage = cv2.resize(image, (224, 224))\n",
    "\timage = np.expand_dims(image, axis=0)\n",
    "\t# return the preprocessed image\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_eps(tensor, eps):\n",
    "\t# clip the values of the tensor to a given range and return it\n",
    "\treturn tf.clip_by_value(tensor, clip_value_min=-eps, clip_value_max=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function will be used when we construct our perturbation vector, ensuring that the noise vector we construct falls within tolerable limits, and most importantly, does not significantly impact the visual quality of the output adversarial image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_targeted_adversaries(model, baseImage, delta, classIdx, target, steps=500):\n",
    "\t# iterate over the number of steps\n",
    "\tfor step in range(0, steps):\n",
    "\t\t# record our gradients\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\t# explicitly indicate that our perturbation vector should\n",
    "\t\t\t# be tracked for gradient updates\n",
    "\t\t\ttape.watch(delta)\n",
    "\t\t\t# add our perturbation vector to the base image and\n",
    "\t\t\t# preprocess the resulting image\n",
    "\t\t\tadversary = preprocess_input(baseImage + delta)\n",
    "\t\t\t# run this newly constructed image tensor through our\n",
    "\t\t\t# model and calculate the loss with respect to the\n",
    "\t\t\t# both the *original* class label and the *target*\n",
    "\t\t\t# class label\n",
    "\t\t\tpredictions = model(adversary, training=False)\n",
    "\t\t\toriginalLoss = -sccLoss(tf.convert_to_tensor([classIdx]),\n",
    "\t\t\t\tpredictions)\n",
    "\t\t\ttargetLoss = sccLoss(tf.convert_to_tensor([target]),\n",
    "\t\t\t\tpredictions)\n",
    "\t\t\ttotalLoss = originalLoss + targetLoss\n",
    "\t\t\t# check to see if we are logging the loss value, and if\n",
    "\t\t\t# so, display it to our terminal\n",
    "\t\t\tif step % 20 == 0:\n",
    "\t\t\t\tprint(\"step: {}, loss: {}...\".format(step, totalLoss.numpy()))\n",
    "\t\t# calculate the gradients of loss with respect to the\n",
    "\t\t# perturbation vector\n",
    "\t\tgradients = tape.gradient(totalLoss, delta)\n",
    "\t\t# update the weights, clip the perturbation vector, and\n",
    "\t\t# update its value\n",
    "\t\toptimizer.apply_gradients([(gradients, delta)])\n",
    "\t\tdelta.assign_add(clip_eps(delta, eps=EPS))\n",
    "\t# return the perturbation vector\n",
    "\treturn delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### originalLoss:\n",
    "Computes the negative sparse categorical cross-entropy loss with respect to the original class label.\n",
    "##### targetLoss: \n",
    "Derives the positive categorical cross-entropy loss with respect to the target class label (i.e., what we want the image adversary to be misclassified as, hence the term targeted adversarial attack). We take the negative/positive signs that way because our objective is to minimize the probability for the true class and maximize the probability of the target class.\n",
    "##### totalLoss: \n",
    "Sum of the original loss and the targeted loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "# construct the argument parser and parse the arguments\n",
    "image_in_path = 'pyimagesearch/pig.jpg'\n",
    "image_out_path = 'pyimagesearch/adversarial.png'\n",
    "#\"ImageNet class ID of the predicted label\"\n",
    "class_idx        = 341 # Hog\n",
    "target_class_idx = 189 # Lakeland_terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image...\n",
      "[INFO] loading finished size is (1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# define the epsilon and learning rate constants\n",
    "EPS = 2 / 255.0\n",
    "LR = 5e-3\n",
    "# load image from disk and preprocess it\n",
    "print(\"[INFO] loading image...\")\n",
    "image = cv2.imread(image_in_path)\n",
    "image = preprocess_image(image)\n",
    "print(\"[INFO] loading finished size is {}\".format(image.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* our epsilon (EPS) value used for clipping tensors when constructing the adversarial image. An EPS value of 2 / 255.0 is a standard value used in adversarial publications and tutorials\n",
    "* A value of LR = 5e-3 was obtained by empirical tuning â€” you may need to update this value when constructing your own adversarial images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading pre-trained ResNet50 model...\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained ResNet50 model for running inference\n",
    "print(\"[INFO] loading pre-trained ResNet50 model...\")\n",
    "model = ResNet50(weights=\"imagenet\")\n",
    "# initialize optimizer and loss function\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "sccLoss = SparseCategoricalCrossentropy()\n",
    "# create a tensor based off the input image and initialize the\n",
    "# perturbation vector (we will update this vector via training)\n",
    "baseImage = tf.constant(image, dtype=tf.float32)\n",
    "delta = tf.Variable(tf.zeros_like(baseImage), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] generating perturbation...\n",
      "step: 0, loss: 16.11762809753418...\n",
      "step: 20, loss: 14.135979652404785...\n",
      "step: 40, loss: 8.167085647583008...\n",
      "step: 60, loss: 4.767631530761719...\n",
      "step: 80, loss: 2.3731980323791504...\n",
      "step: 100, loss: 0.6528186798095703...\n",
      "step: 120, loss: -0.6520843505859375...\n",
      "step: 140, loss: -1.7873167991638184...\n",
      "step: 160, loss: -2.8169145584106445...\n",
      "step: 180, loss: -3.7497940063476562...\n",
      "step: 200, loss: -4.595920085906982...\n",
      "step: 220, loss: -5.411230087280273...\n",
      "step: 240, loss: -6.2791290283203125...\n",
      "step: 260, loss: -7.076803684234619...\n",
      "step: 280, loss: -7.859404563903809...\n",
      "step: 300, loss: -8.648221969604492...\n",
      "step: 320, loss: -9.431211471557617...\n",
      "step: 340, loss: -10.133853912353516...\n",
      "step: 360, loss: -10.722539901733398...\n",
      "step: 380, loss: -11.269286155700684...\n",
      "step: 400, loss: -11.774765014648438...\n",
      "step: 420, loss: -12.33942985534668...\n",
      "step: 440, loss: -12.890159606933594...\n",
      "step: 460, loss: -13.371463775634766...\n",
      "step: 480, loss: -13.876993179321289...\n",
      "[INFO] creating targeted adversarial example...\n"
     ]
    }
   ],
   "source": [
    "# generate the perturbation vector to create an adversarial example\n",
    "print(\"[INFO] generating perturbation...\")\n",
    "deltaUpdated = generate_targeted_adversaries(model, baseImage, delta, class_idx, target_class_idx)\n",
    "# create the adversarial example, swap color channels, and save the\n",
    "# output image to disk\n",
    "print(\"[INFO] creating targeted adversarial example...\")\n",
    "adverImage = (baseImage + deltaUpdated).numpy().squeeze()\n",
    "adverImage = np.clip(adverImage, 0, 255).astype(\"uint8\")\n",
    "adverImage = cv2.cvtColor(adverImage, cv2.COLOR_RGB2BGR)\n",
    "#cv2.imwrite(args[\"output\"], adverImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] running inference on the adversarial example...\n",
      "[INFO] label: water_bottle confidence: 21.59%\n"
     ]
    }
   ],
   "source": [
    "# run inference with this adversarial example, parse the results,\n",
    "# and display the top-1 predicted result\n",
    "print(\"[INFO] running inference on the adversarial example...\")\n",
    "preprocessedImage = preprocess_input(baseImage + deltaUpdated)\n",
    "predictions = model.predict(preprocessedImage)\n",
    "predictions = decode_predictions(predictions, top=3)[0]\n",
    "label = predictions[0][1]\n",
    "confidence = predictions[0][2] * 100\n",
    "print(\"[INFO] label: {} confidence: {:.2f}%\".format(label, confidence))\n",
    "# write the top-most predicted label on the image along with the\n",
    "# confidence score\n",
    "text = \"{}: {:.2f}%\".format(label, confidence)\n",
    "cv2.putText(adverImage, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", adverImage)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
