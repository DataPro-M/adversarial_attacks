{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing adversarial images and attacks with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The \"SparseCategoricalCrossentropy\" computes the categorical cross-entropy loss between the labels and predictions. \n",
    "* By using the sparse version implementation of categorical cross-entropy, we do not have to explicitly one-hot encode our class labels like we would if we were using scikit-learn’s LabelBinarizer or Keras/TensorFlow’s to_categorical utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "\t# swap color channels, resize the input image, and add a batch\n",
    "\t# dimension\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t# image = preprocess_input(image)\n",
    "\timage = cv2.resize(image, (224, 224))\n",
    "\timage = np.expand_dims(image, axis=0)\n",
    "\t# return the preprocessed image\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * leaving out the preprocess_input function call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple helper utility, clip_eps\n",
    "* The goal of this function is to accept an input tensor and then clip any values inside the input to the range [-eps, eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_eps(tensor, eps):\n",
    "\t# clip the values of the tensor to a given range and return it\n",
    "\t# a = -eps if i<-eps else eps if i>eps else i\n",
    "\treturn tf.clip_by_value(tensor, clip_value_min=-eps,clip_value_max=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_adversaries function, which is the meat of our adversarial attack:\n",
    "#### This function accepts four required parameters, including an optional fifth one:\n",
    "\n",
    "* model :  \n",
    "Our ResNet50 model (you could swap in a different pre-trained model such as VGG16, MobileNet, etc. if you prefer).\n",
    "* baseImage (tf.constant):  \n",
    "The original non-perturbed input image that we wish to construct an adversarial attack for, causing our model to misclassify it.\n",
    "* delta (tf.Variable):  \n",
    "Our noise vector, which will be added to the baseImage, ultimately causing the misclassification. We’ll update this delta vector by means of gradient descent.\n",
    "* classIdx (int):  \n",
    "The integer class label index we obtained by running the predict_normal.py\n",
    "script.\n",
    "* steps (int):  \n",
    "Number of gradient descent steps to perform (defaults to 50\n",
    "steps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversaries(model, baseImage, delta, classIdx, steps=50):\n",
    "\t# iterate over the number of steps\n",
    "\tfor step in range(0, steps):\n",
    "\t\t# record our gradients\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\t# explicitly indicate that our perturbation vector should\n",
    "\t\t\t# be tracked for gradient updates\n",
    "\t\t\ttape.watch(delta)\n",
    "\t\t\t# add our perturbation vector to the base image and\n",
    "\t\t\t# preprocess the resulting image\n",
    "\t\t\tadversary = preprocess_input(baseImage + delta)\n",
    "\t\t\t# run this newly constructed image tensor through our\n",
    "\t\t\t# model and calculate the loss with respect to the\n",
    "\t\t\t# *original* class index\n",
    "\t\t\tpredictions = model(adversary, training=False)\n",
    "\t\t\tloss = -sccLoss(tf.convert_to_tensor([classIdx]), predictions)\n",
    "\t\t\t# check to see if we are logging the loss value, and if\n",
    "\t\t\t# so, display it to our terminal\n",
    "\t\t\tif step % 5 == 0:\n",
    "\t\t\t\tprint(\"step: {}, loss: {} ...\".format(step, loss.numpy()))\n",
    "\t\t# calculate the gradients of loss with respect to the\n",
    "\t\t# perturbation vector\n",
    "\t\tgradients = tape.gradient(loss, delta)\n",
    "        \n",
    "\t\t# update the weights, \n",
    "\t\toptimizer.apply_gradients([(gradients, delta)])\n",
    "\t\t# clip the perturbation vector, and update its value \n",
    "\t\t# delta += clip(delta)\n",
    "\t\tdelta.assign_add(clip_eps(delta, eps=EPS))\n",
    "\t# return the perturbation vector\n",
    "\treturn delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It’s far from random.Instead, the pixels in noise vector are “equal to the sign of the elements of the gradient of the cost function with the respect to the input image” (Goodfellow et al.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "# construct the argument parser and parse the arguments\n",
    "image_in_path = 'pyimagesearch/pig.jpg'\n",
    "image_out_path = 'pyimagesearch/adversarial.png'\n",
    "#\"ImageNet class ID of the predicted label\"\n",
    "class_idx = 341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image...\n",
      "[INFO] loading finished size is (1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# define the epsilon and learning rate constants\n",
    "EPS = 2 / 255.0\n",
    "LR = 0.1\n",
    "# load the input image from disk and preprocess it\n",
    "print(\"[INFO] loading image...\")\n",
    "image = cv2.imread(image_in_path)\n",
    "image = preprocess_image(image)\n",
    "print(\"[INFO] loading finished size is {}\".format(image.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* our epsilon (EPS) value used for clipping tensors when constructing the adversarial image. An EPS value of 2 / 255.0 is a standard value used in adversarial publications and tutorials\n",
    "* A value of LR = 0.1 was obtained by empirical tuning — you may need to update this value when constructing your own adversarial images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load our ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading pre-trained ResNet50 model...\n",
      "[INFO] ResNet50 model Loaded\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained ResNet50 model for running inference\n",
    "print(\"[INFO] loading pre-trained ResNet50 model...\")\n",
    "model = ResNet50(weights=\"imagenet\")\n",
    "print(\"[INFO] ResNet50 model Loaded\")\n",
    "# initialize optimizer and loss function\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "sccLoss = SparseCategoricalCrossentropy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s now construct our adversarial image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] generating perturbation...\n",
      "step: 0, loss: -0.0005541696446016431 ...\n",
      "step: 5, loss: -0.0057172346860170364 ...\n",
      "step: 10, loss: -1.9002642631530762 ...\n",
      "step: 15, loss: -8.364564895629883 ...\n",
      "step: 20, loss: -15.761852264404297 ...\n",
      "step: 25, loss: -16.118194580078125 ...\n",
      "step: 30, loss: -16.118194580078125 ...\n",
      "step: 35, loss: -16.118196487426758 ...\n",
      "step: 40, loss: -16.118194580078125 ...\n",
      "step: 45, loss: -16.118194580078125 ...\n"
     ]
    }
   ],
   "source": [
    "# create a \"tensor based off\" the input image and initialize the\n",
    "# perturbation vector (we will update this vector via training)\n",
    "# By default GradientTape will automatically watch any \n",
    "# \"trainable variables\" that are accessed inside the context\n",
    "baseImage = tf.constant(image, dtype=tf.float32)\n",
    "delta = tf.Variable(tf.zeros_like(baseImage), trainable=True)\n",
    "# generate the perturbation vector to create an adversarial example\n",
    "print(\"[INFO] generating perturbation...\")\n",
    "deltaUpdated = generate_adversaries(model, baseImage, delta, class_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The \"generate_adversaries function\" runs, updating the delta pertubration vector along the way, resulting in deltaUpdated, the final noise vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating adversarial example...\n"
     ]
    }
   ],
   "source": [
    "# create the adversarial example, swap color channels, and save the\n",
    "# output image to disk\n",
    "print(\"[INFO] creating adversarial example...\")\n",
    "adverImage = (baseImage + deltaUpdated).numpy().squeeze()\n",
    "adverImage = np.clip(adverImage, 0, 255).astype(\"uint8\")\n",
    "adverImage = cv2.cvtColor(adverImage, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow('adversarial', adverImage)\n",
    "#cv2.waitKey(adverImage)\n",
    "#cv2.imwrite(args[\"output\"], adverImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The real question is, can our newly constructed adversarial image fool our ResNet model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] running inference on the adversarial example...\n",
      "[INFO] label: Ibizan_hound confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# run inference with this adversarial example, parse the results,\n",
    "# and display the top-1 predicted result\n",
    "print(\"[INFO] running inference on the adversarial example...\")\n",
    "preprocessedImage = preprocess_input(baseImage + deltaUpdated)\n",
    "predictions = model.predict(preprocessedImage)\n",
    "predictions = decode_predictions(predictions, top=3)[0]\n",
    "label = predictions[0][1]\n",
    "confidence = predictions[0][2] * 100\n",
    "print(\"[INFO] label: {} confidence: {:.2f}%\".format(label,confidence))\n",
    "# draw the top-most predicted label on the adversarial image along\n",
    "# with the confidence score\n",
    "text = \"{}: {:.2f}%\".format(label, confidence)\n",
    "cv2.putText(adverImage, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", adverImage)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p36)",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
