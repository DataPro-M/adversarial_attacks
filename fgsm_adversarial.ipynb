{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fast Gradient Sign Method (FGSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Taking an input image\n",
    "* Making predictions on the image using a trained CNN\n",
    "* Computing the loss of the prediction based on the true class label\n",
    "* Calculating the gradients of the loss with respect to the input image\n",
    "* Computing the sign of the gradient\n",
    "* Using the signed gradient to construct the output adversarial image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the Fast Gradient Sign Method work?\n",
    "* Essentially, FGSM computes the gradients of a loss function (e.g., mean-squared error or categorical cross-entropy) with respect to the input image and then uses the sign of the gradients to create a new image (i.e., the adversarial image) that maximizes the loss\n",
    "![alt text](pyimagesearch/fgsm_equation.png \"Title\")\n",
    "where:\n",
    "\n",
    "* adv_x: Our output adversarial image\n",
    "* x: The original input image\n",
    "* y: The ground-truth label of the input image\n",
    "* epsilon: Small value we multiply the signed gradients by to ensure the perturbations are small enough that the human eye cannot detect them but large enough that they fool the neural network\n",
    "* theta: Our neural network model\n",
    "* J: The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from pyimagesearch.simplecnn import SimpleCNN\n",
    "from pyimagesearch.fgsm import generate_image_adversary\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the MNIST dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "# load MNIST dataset and scale the pixel values to the range [0, 1]\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "# add a channel dimension to the images\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "# one-hot encode our labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the MNIST dataset by:\n",
    "\n",
    "* Scaling the pixel intensities from the range [0, 255] to [0, 1]\n",
    "* Adding a batch dimension to the images\n",
    "* One-hot encoding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize our SimpleCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.1978 - accuracy: 0.9402 - val_loss: 0.0572 - val_accuracy: 0.9811\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0752 - accuracy: 0.9774 - val_loss: 0.0429 - val_accuracy: 0.9871\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0580 - accuracy: 0.9824 - val_loss: 0.0396 - val_accuracy: 0.9876\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.0413 - val_accuracy: 0.9866\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0398 - accuracy: 0.9873 - val_loss: 0.0361 - val_accuracy: 0.9879\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0337 - accuracy: 0.9894 - val_loss: 0.0363 - val_accuracy: 0.9882\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0279 - accuracy: 0.9910 - val_loss: 0.0366 - val_accuracy: 0.9888\n",
      "Epoch 8/10\n",
      " 4032/60000 [=>............................] - ETA: 3s - loss: 0.0327 - accuracy: 0.9888"
     ]
    }
   ],
   "source": [
    "# initialize our optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=1e-3)\n",
    "model = SimpleCNN.build(width=28, height=28, depth=1, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the simple CNN on MNIST\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(trainX, trainY,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tbatch_size=64,\n",
    "\tepochs=10,\n",
    "\tverbose=1)\n",
    "# make predictions on the testing set for the model trained on\n",
    "# non-adversarial images\n",
    "(loss, acc) = model.evaluate(x=testX, y=testY, verbose=0)\n",
    "print(\"[INFO] loss: {:.4f}, acc: {:.4f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate some adversarial images using the FGSM now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over a sample of our testing images\n",
    "for i in np.random.choice(np.arange(0, len(testX)), size=(10,)):\n",
    "\t# grab the current image and label\n",
    "\timage = testX[i]\n",
    "\tlabel = testY[i]\n",
    "\t# generate an image adversary for the current image and make\n",
    "\t# a prediction on the adversary\n",
    "\tadversary = generate_image_adversary(model,\n",
    "\t\timage.reshape(1, 28, 28, 1), label, eps=0.1)\n",
    "\tpred = model.predict(adversary)\n",
    "\t# scale both the original image and adversary to the range\n",
    "\t# [0, 255] and convert them to an unsigned 8-bit integers\n",
    "\tadversary = adversary.reshape((28, 28)) * 255\n",
    "\tadversary = np.clip(adversary, 0, 255).astype(\"uint8\")\n",
    "\timage = image.reshape((28, 28)) * 255\n",
    "\timage = image.astype(\"uint8\")\n",
    "\t# convert the image and adversarial image from grayscale to three\n",
    "\t# channel (so we can draw on them)\n",
    "\timage = np.dstack([image] * 3)\n",
    "\tadversary = np.dstack([adversary] * 3)\n",
    "\t# resize the images so we can better visualize them\n",
    "\timage = cv2.resize(image, (96, 96))\n",
    "\tadversary = cv2.resize(adversary, (96, 96))\n",
    "\t# determine the predicted label for both the original image and\n",
    "\t# adversarial image\n",
    "\timagePred = label.argmax()\n",
    "\tadversaryPred = pred[0].argmax()\n",
    "\tcolor = (0, 255, 0)\n",
    "\t# if the image prediction does not match the adversarial\n",
    "\t# prediction then update the color\n",
    "\tif imagePred != adversaryPred:\n",
    "\t\tcolor = (0, 0, 255)\n",
    "\t# draw the predictions on the respective output images\n",
    "\tcv2.putText(image, str(imagePred), (2, 25),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.95, (0, 255, 0), 2)\n",
    "\tcv2.putText(adversary, str(adversaryPred), (2, 25),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.95, color, 2)\n",
    "\t# stack the two images horizontally and then show the original\n",
    "\t# image and adversarial image\n",
    "\toutput = np.hstack([image, adversary])\n",
    "\tprint(\"Real image is {} while it is predicted as {}\".format(str(imagePred), str(adversaryPred)))\n",
    "\t#cv2.imshow(\"FGSM Adversarial Images\", output)\n",
    "\t#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, take note of the image.reshape call where we are ensuring \n",
    "the image has a shape of (1, 28, 28, 1). These values are:\n",
    "\n",
    "* 1  : Batch dimension; we’re working with a single image here, so the value is trivially set to one.\n",
    "* 28 : Height of the image\n",
    "* 28 : Width of the image\n",
    "* 1  : Number of channels in the image (MNIST images are grayscale, hence only one channel)\n",
    "\n",
    "our preprocessing steps included scaling our training/testing images from the range [0, 255] to [0, 1] — to visualize our images with OpenCV, we now need to undo these preprocessing operations.\n",
    "\n",
    "Initialize the color of our labels to be “green” (Line 76) if both the imagePred\n",
    "and adversaryPred are equal. This will happen if our model can correctly label the adversarial image. Otherwise, we’ll update our prediction color to be red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p36)",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
